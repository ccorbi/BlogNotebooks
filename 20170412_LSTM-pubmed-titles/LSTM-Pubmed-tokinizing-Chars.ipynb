{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pubmed = pickle.load( open( \"./Pubmed/bins/pubmed_1998.p\", \"rb\" ), encoding=\"latin1\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubmedid</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>jounal</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600465</th>\n",
       "      <td>11655160</td>\n",
       "      <td>1998</td>\n",
       "      <td>Controversies in transfusion medicine: directe...</td>\n",
       "      <td>Transfusion</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600787</th>\n",
       "      <td>11648630</td>\n",
       "      <td>1998</td>\n",
       "      <td>W v. Egdell.</td>\n",
       "      <td>The all England law reports</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601826</th>\n",
       "      <td>12293749</td>\n",
       "      <td>1998</td>\n",
       "      <td>AIDS as a political issue: working with the se...</td>\n",
       "      <td>Community development journal</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602678</th>\n",
       "      <td>12158206</td>\n",
       "      <td>1998</td>\n",
       "      <td>[Decree No. 70-89 of 23 November 1989, the For...</td>\n",
       "      <td>Diario de Centro América (Guatemala, Guatemala...</td>\n",
       "      <td>spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630408</th>\n",
       "      <td>11660372</td>\n",
       "      <td>1998</td>\n",
       "      <td>Refusal of medical treatment in \"captive\" circ...</td>\n",
       "      <td>The Canadian bar review</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630409</th>\n",
       "      <td>11652468</td>\n",
       "      <td>1998</td>\n",
       "      <td>The challenge of biotechnology.</td>\n",
       "      <td>Yale law &amp; policy review</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630410</th>\n",
       "      <td>11655150</td>\n",
       "      <td>1998</td>\n",
       "      <td>Regulating human gene therapy.</td>\n",
       "      <td>West Virginia law review</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630411</th>\n",
       "      <td>11660404</td>\n",
       "      <td>1998</td>\n",
       "      <td>Informed consent to psychotherapy: current pra...</td>\n",
       "      <td>Law and psychology review</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630412</th>\n",
       "      <td>11660412</td>\n",
       "      <td>1998</td>\n",
       "      <td>Treatment dilemmas for imperiled newborns: why...</td>\n",
       "      <td>Southern California law review</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630413</th>\n",
       "      <td>11645795</td>\n",
       "      <td>1998</td>\n",
       "      <td>Governmental regulation of the investigation o...</td>\n",
       "      <td>Minerva</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pubmedid  year                                              title  \\\n",
       "600465  11655160  1998  Controversies in transfusion medicine: directe...   \n",
       "600787  11648630  1998                                       W v. Egdell.   \n",
       "601826  12293749  1998  AIDS as a political issue: working with the se...   \n",
       "602678  12158206  1998  [Decree No. 70-89 of 23 November 1989, the For...   \n",
       "630408  11660372  1998  Refusal of medical treatment in \"captive\" circ...   \n",
       "630409  11652468  1998                    The challenge of biotechnology.   \n",
       "630410  11655150  1998                     Regulating human gene therapy.   \n",
       "630411  11660404  1998  Informed consent to psychotherapy: current pra...   \n",
       "630412  11660412  1998  Treatment dilemmas for imperiled newborns: why...   \n",
       "630413  11645795  1998  Governmental regulation of the investigation o...   \n",
       "\n",
       "                                                   jounal lang  \n",
       "600465                                        Transfusion  eng  \n",
       "600787                        The all England law reports  eng  \n",
       "601826                      Community development journal  eng  \n",
       "602678  Diario de Centro América (Guatemala, Guatemala...  spa  \n",
       "630408                            The Canadian bar review  eng  \n",
       "630409                           Yale law & policy review  eng  \n",
       "630410                           West Virginia law review  eng  \n",
       "630411                          Law and psychology review  eng  \n",
       "630412                     Southern California law review  eng  \n",
       "630413                                            Minerva  eng  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, row in pubmed.iterrows():\n",
    "    for c in row['title'].lower():\n",
    "        if c not in values:\n",
    "            values[c] =1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word to id dict\n",
    "chr_to_id = {token: idx for idx, token in enumerate(values)}\n",
    "id_to_chr = {idx: token for idx, token in enumerate(values)}\n",
    "\n",
    "\n",
    "# convert token lists to token-id lists, e.g. [[1, 2], [2, 2]] here\n",
    "#token_ids = [[word_to_id[token] for token in tokens_doc] for tokens_doc in tokens_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chr_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a continuous text woth all the titles. \n",
    "pubmed_corpus = list()\n",
    "for i, row in pubmed.iterrows():\n",
    "    pubmed_corpus.append(row['title'].lower())\n",
    "\n",
    "pubmed_corpus = ' '.join(pubmed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'controversies in transfusion medicine: directed bl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pubmed_corpus[0:0 + 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_chars_corpus = len(pubmed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39772577"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chars_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1988627"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(range(0, n_chars_corpus - 50, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 1988627\n"
     ]
    }
   ],
   "source": [
    "# float window 5 to 1\n",
    "n_chars_corpus = len(pubmed_corpus)\n",
    "max_len = 50\n",
    "overlap = 20\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars_corpus - max_len, overlap):\n",
    "\tseq_in = pubmed_corpus[i:i + max_len]\n",
    "\tseq_target = pubmed_corpus[i + max_len]\n",
    "\tdataX.append([chr_to_id[char] for char in seq_in])\n",
    "\tdataY.append(chr_to_id[seq_target])\n",
    "n_patterns = len(dataX)\n",
    "print( \"Total Patterns: {}\".format( n_patterns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.zeros((n_patterns, max_len, len(chr_to_id)), dtype=np.bool)\n",
    "y = np.zeros((n_patterns, len(chr_to_id)), dtype=np.bool)\n",
    "\n",
    "\n",
    "\n",
    "# vectorize in LSTM format   \n",
    "for idx,title in enumerate(dataX):\n",
    "    for jdx, c in enumerate(title):\n",
    "        X[idx, jdx, c] = 1\n",
    "    y[idx, dataY[idx]] = 1\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1988627, 50, 101)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1988627, 101)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(max_len, len(chr_to_id))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chr_to_id)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import  TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard.set_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               366592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 101)               25957     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 101)               0         \n",
      "=================================================================\n",
      "Total params: 392,549\n",
      "Trainable params: 392,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccorbi/anaconda3/lib/python3.5/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 2.1029Epoch 00000: loss improved from inf to 2.10285, saving model to 1998_char-00-2.1028.hdf5\n",
      "1988627/1988627 [==============================] - 935s - loss: 2.1028   \n",
      "Epoch 2/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.6683Epoch 00001: loss improved from 2.10285 to 1.66828, saving model to 1998_char-01-1.6683.hdf5\n",
      "1988627/1988627 [==============================] - 933s - loss: 1.6683   \n",
      "Epoch 3/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.5281Epoch 00002: loss improved from 1.66828 to 1.52810, saving model to 1998_char-02-1.5281.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.5281   \n",
      "Epoch 4/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.4587Epoch 00003: loss improved from 1.52810 to 1.45874, saving model to 1998_char-03-1.4587.hdf5\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.4587   \n",
      "Epoch 5/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.4160Epoch 00004: loss improved from 1.45874 to 1.41602, saving model to 1998_char-04-1.4160.hdf5\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.4160   \n",
      "Epoch 6/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.3853Epoch 00005: loss improved from 1.41602 to 1.38529, saving model to 1998_char-05-1.3853.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.3853   \n",
      "Epoch 7/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.3634Epoch 00006: loss improved from 1.38529 to 1.36340, saving model to 1998_char-06-1.3634.hdf5\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.3634   \n",
      "Epoch 8/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.3447Epoch 00007: loss improved from 1.36340 to 1.34466, saving model to 1998_char-07-1.3447.hdf5\n",
      "1988627/1988627 [==============================] - 928s - loss: 1.3447   \n",
      "Epoch 9/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.3302Epoch 00008: loss improved from 1.34466 to 1.33023, saving model to 1998_char-08-1.3302.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.3302   \n",
      "Epoch 10/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.3177Epoch 00009: loss improved from 1.33023 to 1.31775, saving model to 1998_char-09-1.3177.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.3177   \n",
      "Epoch 11/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.3071Epoch 00010: loss improved from 1.31775 to 1.30712, saving model to 1998_char-10-1.3071.hdf5\n",
      "1988627/1988627 [==============================] - 928s - loss: 1.3071   \n",
      "Epoch 12/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2980Epoch 00011: loss improved from 1.30712 to 1.29803, saving model to 1998_char-11-1.2980.hdf5\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.2980   \n",
      "Epoch 13/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2900Epoch 00012: loss improved from 1.29803 to 1.29005, saving model to 1998_char-12-1.2900.hdf5\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.2900   \n",
      "Epoch 14/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2830Epoch 00013: loss improved from 1.29005 to 1.28300, saving model to 1998_char-13-1.2830.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.2830   \n",
      "Epoch 15/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2766Epoch 00014: loss improved from 1.28300 to 1.27662, saving model to 1998_char-14-1.2766.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.2766   \n",
      "Epoch 16/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2712Epoch 00015: loss improved from 1.27662 to 1.27115, saving model to 1998_char-15-1.2712.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.2712   \n",
      "Epoch 17/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2657Epoch 00016: loss improved from 1.27115 to 1.26571, saving model to 1998_char-16-1.2657.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.2657   \n",
      "Epoch 18/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2602Epoch 00017: loss improved from 1.26571 to 1.26017, saving model to 1998_char-17-1.2602.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.2602   \n",
      "Epoch 19/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2559Epoch 00018: loss improved from 1.26017 to 1.25591, saving model to 1998_char-18-1.2559.hdf5\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.2559   \n",
      "Epoch 20/20\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2523Epoch 00019: loss improved from 1.25591 to 1.25232, saving model to 1998_char-19-1.2523.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.2523   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0dc0601828>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('-' * 50)\n",
    "fileName= '1998_char'\n",
    "filepath = fileName+\"-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "model.fit(X, y, batch_size=256, nb_epoch=20, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccorbi/anaconda3/lib/python3.5/site-packages/keras/models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2486Epoch 00000: loss improved from 1.25232 to 1.24858, saving model to 1998_char-00-1.2486.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.2486   \n",
      "Epoch 2/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2451Epoch 00001: loss improved from 1.24858 to 1.24509, saving model to 1998_char-01-1.2451.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.2451   \n",
      "Epoch 3/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2417Epoch 00002: loss improved from 1.24509 to 1.24171, saving model to 1998_char-02-1.2417.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.2417   \n",
      "Epoch 4/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2386Epoch 00003: loss improved from 1.24171 to 1.23862, saving model to 1998_char-03-1.2386.hdf5\n",
      "1988627/1988627 [==============================] - 921s - loss: 1.2386   \n",
      "Epoch 5/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2354Epoch 00004: loss improved from 1.23862 to 1.23539, saving model to 1998_char-04-1.2354.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.2354   \n",
      "Epoch 6/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2334Epoch 00005: loss improved from 1.23539 to 1.23344, saving model to 1998_char-05-1.2334.hdf5\n",
      "1988627/1988627 [==============================] - 921s - loss: 1.2334   \n",
      "Epoch 7/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2303Epoch 00006: loss improved from 1.23344 to 1.23029, saving model to 1998_char-06-1.2303.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.2303   \n",
      "Epoch 8/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2275Epoch 00007: loss improved from 1.23029 to 1.22754, saving model to 1998_char-07-1.2275.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.2275   \n",
      "Epoch 9/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2262Epoch 00008: loss improved from 1.22754 to 1.22616, saving model to 1998_char-08-1.2262.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.2262   \n",
      "Epoch 10/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2234Epoch 00009: loss improved from 1.22616 to 1.22341, saving model to 1998_char-09-1.2234.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.2234   \n",
      "Epoch 11/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2213Epoch 00010: loss improved from 1.22341 to 1.22133, saving model to 1998_char-10-1.2213.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.2213   \n",
      "Epoch 12/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2205Epoch 00011: loss improved from 1.22133 to 1.22048, saving model to 1998_char-11-1.2205.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.2205   \n",
      "Epoch 13/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2179Epoch 00012: loss improved from 1.22048 to 1.21790, saving model to 1998_char-12-1.2179.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.2179   \n",
      "Epoch 14/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2159Epoch 00013: loss improved from 1.21790 to 1.21594, saving model to 1998_char-13-1.2159.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.2159   \n",
      "Epoch 15/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2141Epoch 00014: loss improved from 1.21594 to 1.21408, saving model to 1998_char-14-1.2141.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.2141   \n",
      "Epoch 16/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2122Epoch 00015: loss improved from 1.21408 to 1.21220, saving model to 1998_char-15-1.2122.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.2122   \n",
      "Epoch 17/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2106Epoch 00016: loss improved from 1.21220 to 1.21060, saving model to 1998_char-16-1.2106.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.2106   \n",
      "Epoch 18/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2094Epoch 00017: loss improved from 1.21060 to 1.20942, saving model to 1998_char-17-1.2094.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.2094   \n",
      "Epoch 19/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2079Epoch 00018: loss improved from 1.20942 to 1.20786, saving model to 1998_char-18-1.2079.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.2079   \n",
      "Epoch 20/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2061Epoch 00019: loss improved from 1.20786 to 1.20614, saving model to 1998_char-19-1.2061.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.2061   \n",
      "Epoch 21/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2048Epoch 00020: loss improved from 1.20614 to 1.20482, saving model to 1998_char-20-1.2048.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.2048   \n",
      "Epoch 22/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2037Epoch 00021: loss improved from 1.20482 to 1.20372, saving model to 1998_char-21-1.2037.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.2037   \n",
      "Epoch 23/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2021Epoch 00022: loss improved from 1.20372 to 1.20205, saving model to 1998_char-22-1.2020.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.2020   \n",
      "Epoch 24/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2001Epoch 00023: loss improved from 1.20205 to 1.20012, saving model to 1998_char-23-1.2001.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.2001   \n",
      "Epoch 25/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.2002Epoch 00024: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.2002   \n",
      "Epoch 26/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1986Epoch 00025: loss improved from 1.20012 to 1.19865, saving model to 1998_char-25-1.1986.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1986   \n",
      "Epoch 27/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1971Epoch 00026: loss improved from 1.19865 to 1.19706, saving model to 1998_char-26-1.1971.hdf5\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1971   \n",
      "Epoch 28/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1958Epoch 00027: loss improved from 1.19706 to 1.19585, saving model to 1998_char-27-1.1958.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1958   \n",
      "Epoch 29/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1944Epoch 00028: loss improved from 1.19585 to 1.19437, saving model to 1998_char-28-1.1944.hdf5\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1944   \n",
      "Epoch 30/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1937Epoch 00029: loss improved from 1.19437 to 1.19371, saving model to 1998_char-29-1.1937.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1937   \n",
      "Epoch 31/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1926Epoch 00030: loss improved from 1.19371 to 1.19264, saving model to 1998_char-30-1.1926.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1926   \n",
      "Epoch 32/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1931Epoch 00031: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1931   \n",
      "Epoch 33/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1911Epoch 00032: loss improved from 1.19264 to 1.19109, saving model to 1998_char-32-1.1911.hdf5\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1911   \n",
      "Epoch 34/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1901Epoch 00033: loss improved from 1.19109 to 1.19012, saving model to 1998_char-33-1.1901.hdf5\n",
      "1988627/1988627 [==============================] - 921s - loss: 1.1901   \n",
      "Epoch 35/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1889Epoch 00034: loss improved from 1.19012 to 1.18890, saving model to 1998_char-34-1.1889.hdf5\n",
      "1988627/1988627 [==============================] - 920s - loss: 1.1889   \n",
      "Epoch 36/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1882Epoch 00035: loss improved from 1.18890 to 1.18818, saving model to 1998_char-35-1.1882.hdf5\n",
      "1988627/1988627 [==============================] - 921s - loss: 1.1882   \n",
      "Epoch 37/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1871Epoch 00036: loss improved from 1.18818 to 1.18714, saving model to 1998_char-36-1.1871.hdf5\n",
      "1988627/1988627 [==============================] - 921s - loss: 1.1871   \n",
      "Epoch 38/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1859Epoch 00037: loss improved from 1.18714 to 1.18590, saving model to 1998_char-37-1.1859.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1859   \n",
      "Epoch 39/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1852Epoch 00038: loss improved from 1.18590 to 1.18523, saving model to 1998_char-38-1.1852.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1852   \n",
      "Epoch 40/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1840Epoch 00039: loss improved from 1.18523 to 1.18404, saving model to 1998_char-39-1.1840.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1840   \n",
      "Epoch 41/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1833Epoch 00040: loss improved from 1.18404 to 1.18332, saving model to 1998_char-40-1.1833.hdf5\n",
      "1988627/1988627 [==============================] - 921s - loss: 1.1833   \n",
      "Epoch 42/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1828Epoch 00041: loss improved from 1.18332 to 1.18284, saving model to 1998_char-41-1.1828.hdf5\n",
      "1988627/1988627 [==============================] - 921s - loss: 1.1828   \n",
      "Epoch 43/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1821Epoch 00042: loss improved from 1.18284 to 1.18211, saving model to 1998_char-42-1.1821.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1821   \n",
      "Epoch 44/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1813Epoch 00043: loss improved from 1.18211 to 1.18132, saving model to 1998_char-43-1.1813.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1813   \n",
      "Epoch 45/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1801Epoch 00044: loss improved from 1.18132 to 1.18013, saving model to 1998_char-44-1.1801.hdf5\n",
      "1988627/1988627 [==============================] - 921s - loss: 1.1801   \n",
      "Epoch 46/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1793Epoch 00045: loss improved from 1.18013 to 1.17928, saving model to 1998_char-45-1.1793.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1793   \n",
      "Epoch 47/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1783Epoch 00046: loss improved from 1.17928 to 1.17829, saving model to 1998_char-46-1.1783.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1783   \n",
      "Epoch 48/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1775Epoch 00047: loss improved from 1.17829 to 1.17752, saving model to 1998_char-47-1.1775.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1775   \n",
      "Epoch 49/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1772Epoch 00048: loss improved from 1.17752 to 1.17715, saving model to 1998_char-48-1.1772.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1772   \n",
      "Epoch 50/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1763Epoch 00049: loss improved from 1.17715 to 1.17625, saving model to 1998_char-49-1.1763.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1763   \n",
      "Epoch 51/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1759Epoch 00050: loss improved from 1.17625 to 1.17588, saving model to 1998_char-50-1.1759.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1759   \n",
      "Epoch 52/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1745Epoch 00051: loss improved from 1.17588 to 1.17446, saving model to 1998_char-51-1.1745.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1745   \n",
      "Epoch 53/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1747Epoch 00052: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1747   \n",
      "Epoch 54/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1741Epoch 00053: loss improved from 1.17446 to 1.17414, saving model to 1998_char-53-1.1741.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1741   \n",
      "Epoch 55/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1727Epoch 00054: loss improved from 1.17414 to 1.17274, saving model to 1998_char-54-1.1727.hdf5\n",
      "1988627/1988627 [==============================] - 921s - loss: 1.1727   \n",
      "Epoch 56/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1719Epoch 00055: loss improved from 1.17274 to 1.17188, saving model to 1998_char-55-1.1719.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1719   \n",
      "Epoch 57/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1722Epoch 00056: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1722   \n",
      "Epoch 58/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1709Epoch 00057: loss improved from 1.17188 to 1.17086, saving model to 1998_char-57-1.1709.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1709   \n",
      "Epoch 59/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1704Epoch 00058: loss improved from 1.17086 to 1.17037, saving model to 1998_char-58-1.1704.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1704   \n",
      "Epoch 60/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1700Epoch 00059: loss improved from 1.17037 to 1.17001, saving model to 1998_char-59-1.1700.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1700   \n",
      "Epoch 61/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1694Epoch 00060: loss improved from 1.17001 to 1.16945, saving model to 1998_char-60-1.1694.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1694   \n",
      "Epoch 62/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1693Epoch 00061: loss improved from 1.16945 to 1.16927, saving model to 1998_char-61-1.1693.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1693   \n",
      "Epoch 63/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1676Epoch 00062: loss improved from 1.16927 to 1.16762, saving model to 1998_char-62-1.1676.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1676   \n",
      "Epoch 64/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1674Epoch 00063: loss improved from 1.16762 to 1.16739, saving model to 1998_char-63-1.1674.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1674   \n",
      "Epoch 65/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1666Epoch 00064: loss improved from 1.16739 to 1.16657, saving model to 1998_char-64-1.1666.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1666   \n",
      "Epoch 66/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1661Epoch 00065: loss improved from 1.16657 to 1.16606, saving model to 1998_char-65-1.1661.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988627/1988627 [==============================] - 924s - loss: 1.1661   \n",
      "Epoch 67/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1654Epoch 00066: loss improved from 1.16606 to 1.16541, saving model to 1998_char-66-1.1654.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1654   \n",
      "Epoch 68/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1649Epoch 00067: loss improved from 1.16541 to 1.16487, saving model to 1998_char-67-1.1649.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1649   \n",
      "Epoch 69/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1641Epoch 00068: loss improved from 1.16487 to 1.16411, saving model to 1998_char-68-1.1641.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1641   \n",
      "Epoch 70/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1638Epoch 00069: loss improved from 1.16411 to 1.16379, saving model to 1998_char-69-1.1638.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1638   \n",
      "Epoch 71/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1633Epoch 00070: loss improved from 1.16379 to 1.16325, saving model to 1998_char-70-1.1633.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1633   \n",
      "Epoch 72/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1637Epoch 00071: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1637   \n",
      "Epoch 73/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1630Epoch 00072: loss improved from 1.16325 to 1.16299, saving model to 1998_char-72-1.1630.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1630   \n",
      "Epoch 74/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1621Epoch 00073: loss improved from 1.16299 to 1.16208, saving model to 1998_char-73-1.1621.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1621   \n",
      "Epoch 75/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1617Epoch 00074: loss improved from 1.16208 to 1.16167, saving model to 1998_char-74-1.1617.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1617   \n",
      "Epoch 76/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1607Epoch 00075: loss improved from 1.16167 to 1.16068, saving model to 1998_char-75-1.1607.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1607   \n",
      "Epoch 77/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1608Epoch 00076: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1608   \n",
      "Epoch 78/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1607Epoch 00077: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1607   \n",
      "Epoch 79/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1592Epoch 00078: loss improved from 1.16068 to 1.15924, saving model to 1998_char-78-1.1592.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1592   \n",
      "Epoch 80/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1591Epoch 00079: loss improved from 1.15924 to 1.15907, saving model to 1998_char-79-1.1591.hdf5\n",
      "1988627/1988627 [==============================] - 929s - loss: 1.1591   \n",
      "Epoch 81/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1589Epoch 00080: loss improved from 1.15907 to 1.15889, saving model to 1998_char-80-1.1589.hdf5\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1589   \n",
      "Epoch 82/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1583Epoch 00081: loss improved from 1.15889 to 1.15826, saving model to 1998_char-81-1.1583.hdf5\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1583   \n",
      "Epoch 83/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1572Epoch 00082: loss improved from 1.15826 to 1.15715, saving model to 1998_char-82-1.1572.hdf5\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1572   \n",
      "Epoch 84/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1576Epoch 00083: loss did not improve\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.1576   \n",
      "Epoch 85/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1572Epoch 00084: loss did not improve\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.1572   \n",
      "Epoch 86/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1562Epoch 00085: loss improved from 1.15715 to 1.15617, saving model to 1998_char-85-1.1562.hdf5\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1562   \n",
      "Epoch 87/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1556Epoch 00086: loss improved from 1.15617 to 1.15561, saving model to 1998_char-86-1.1556.hdf5\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.1556   \n",
      "Epoch 88/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1551Epoch 00087: loss improved from 1.15561 to 1.15507, saving model to 1998_char-87-1.1551.hdf5\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.1551   \n",
      "Epoch 89/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1555Epoch 00088: loss did not improve\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.1555   \n",
      "Epoch 90/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1548Epoch 00089: loss improved from 1.15507 to 1.15478, saving model to 1998_char-89-1.1548.hdf5\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.1548   \n",
      "Epoch 91/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1546Epoch 00090: loss improved from 1.15478 to 1.15458, saving model to 1998_char-90-1.1546.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.1546   \n",
      "Epoch 92/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1538Epoch 00091: loss improved from 1.15458 to 1.15377, saving model to 1998_char-91-1.1538.hdf5\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1538   \n",
      "Epoch 93/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1534Epoch 00092: loss improved from 1.15377 to 1.15341, saving model to 1998_char-92-1.1534.hdf5\n",
      "1988627/1988627 [==============================] - 928s - loss: 1.1534   \n",
      "Epoch 94/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1535Epoch 00093: loss did not improve\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.1535   \n",
      "Epoch 95/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1528Epoch 00094: loss improved from 1.15341 to 1.15278, saving model to 1998_char-94-1.1528.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.1528   \n",
      "Epoch 96/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1524Epoch 00095: loss improved from 1.15278 to 1.15238, saving model to 1998_char-95-1.1524.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.1524   \n",
      "Epoch 97/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1518Epoch 00096: loss improved from 1.15238 to 1.15183, saving model to 1998_char-96-1.1518.hdf5\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.1518   \n",
      "Epoch 98/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1508Epoch 00097: loss improved from 1.15183 to 1.15083, saving model to 1998_char-97-1.1508.hdf5\n",
      "1988627/1988627 [==============================] - 927s - loss: 1.1508   \n",
      "Epoch 99/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1510Epoch 00098: loss did not improve\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.1510   \n",
      "Epoch 100/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1518Epoch 00099: loss did not improve\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.1518   \n",
      "Epoch 101/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1506Epoch 00100: loss improved from 1.15083 to 1.15064, saving model to 1998_char-100-1.1506.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1506   \n",
      "Epoch 102/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1498Epoch 00101: loss improved from 1.15064 to 1.14981, saving model to 1998_char-101-1.1498.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1498   \n",
      "Epoch 103/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1500Epoch 00102: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1500   \n",
      "Epoch 104/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1496Epoch 00103: loss improved from 1.14981 to 1.14963, saving model to 1998_char-103-1.1496.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1496   \n",
      "Epoch 105/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1491Epoch 00104: loss improved from 1.14963 to 1.14909, saving model to 1998_char-104-1.1491.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1491   \n",
      "Epoch 106/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1490Epoch 00105: loss improved from 1.14909 to 1.14901, saving model to 1998_char-105-1.1490.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1490   \n",
      "Epoch 107/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1484Epoch 00106: loss improved from 1.14901 to 1.14844, saving model to 1998_char-106-1.1484.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1484   \n",
      "Epoch 108/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1476Epoch 00107: loss improved from 1.14844 to 1.14759, saving model to 1998_char-107-1.1476.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1476   \n",
      "Epoch 109/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1475Epoch 00108: loss improved from 1.14759 to 1.14746, saving model to 1998_char-108-1.1475.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1475   \n",
      "Epoch 110/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1471Epoch 00109: loss improved from 1.14746 to 1.14707, saving model to 1998_char-109-1.1471.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1471   \n",
      "Epoch 111/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1469Epoch 00110: loss improved from 1.14707 to 1.14688, saving model to 1998_char-110-1.1469.hdf5\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1469   \n",
      "Epoch 112/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1468Epoch 00111: loss improved from 1.14688 to 1.14676, saving model to 1998_char-111-1.1468.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1468   \n",
      "Epoch 113/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1410Epoch 00112: loss improved from 1.14676 to 1.14103, saving model to 1998_char-112-1.1410.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1410   \n",
      "Epoch 114/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1398Epoch 00113: loss improved from 1.14103 to 1.13978, saving model to 1998_char-113-1.1398.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1398   \n",
      "Epoch 115/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1493Epoch 00114: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1493   \n",
      "Epoch 116/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1618Epoch 00115: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1619   \n",
      "Epoch 117/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1565Epoch 00116: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1565   \n",
      "Epoch 118/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1660Epoch 00117: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1660   \n",
      "Epoch 119/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1796Epoch 00118: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1796   \n",
      "Epoch 120/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1830Epoch 00119: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1830   \n",
      "Epoch 121/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1744Epoch 00120: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1744   \n",
      "Epoch 122/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1676Epoch 00121: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1676   \n",
      "Epoch 123/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1636Epoch 00122: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1636   \n",
      "Epoch 124/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1591Epoch 00123: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1591   \n",
      "Epoch 125/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1560Epoch 00124: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1561   \n",
      "Epoch 126/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1518Epoch 00125: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1518   \n",
      "Epoch 127/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1495Epoch 00126: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1495   \n",
      "Epoch 128/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1485Epoch 00127: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1485   \n",
      "Epoch 129/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1470Epoch 00128: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1470   \n",
      "Epoch 130/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1459Epoch 00129: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1459   \n",
      "Epoch 131/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1464Epoch 00130: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1464   \n",
      "Epoch 132/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1453Epoch 00131: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1453   \n",
      "Epoch 133/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1444Epoch 00132: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1444   \n",
      "Epoch 134/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1431Epoch 00133: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1430   \n",
      "Epoch 135/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1399Epoch 00134: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1399   \n",
      "Epoch 136/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1432Epoch 00135: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1432   \n",
      "Epoch 137/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1421Epoch 00136: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1421   \n",
      "Epoch 138/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1417Epoch 00137: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1417   \n",
      "Epoch 139/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1408Epoch 00138: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1407   \n",
      "Epoch 140/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1405Epoch 00139: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1404   \n",
      "Epoch 141/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1405Epoch 00140: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1405   \n",
      "Epoch 142/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1401Epoch 00141: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1401   \n",
      "Epoch 143/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1397Epoch 00142: loss improved from 1.13978 to 1.13969, saving model to 1998_char-142-1.1397.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1397   \n",
      "Epoch 144/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1402Epoch 00143: loss did not improve\n",
      "1988627/1988627 [==============================] - 921s - loss: 1.1402   \n",
      "Epoch 145/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1390Epoch 00144: loss improved from 1.13969 to 1.13898, saving model to 1998_char-144-1.1390.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1390   \n",
      "Epoch 146/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1385Epoch 00145: loss improved from 1.13898 to 1.13851, saving model to 1998_char-145-1.1385.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1385   \n",
      "Epoch 147/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1384Epoch 00146: loss improved from 1.13851 to 1.13836, saving model to 1998_char-146-1.1384.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1384   \n",
      "Epoch 148/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1379Epoch 00147: loss improved from 1.13836 to 1.13791, saving model to 1998_char-147-1.1379.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1379   \n",
      "Epoch 149/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1370Epoch 00148: loss improved from 1.13791 to 1.13699, saving model to 1998_char-148-1.1370.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1370   \n",
      "Epoch 150/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1378Epoch 00149: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1378   \n",
      "Epoch 151/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1352Epoch 00150: loss improved from 1.13699 to 1.13518, saving model to 1998_char-150-1.1352.hdf5\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1352   \n",
      "Epoch 152/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1301Epoch 00151: loss improved from 1.13518 to 1.13007, saving model to 1998_char-151-1.1301.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1301   \n",
      "Epoch 153/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1372Epoch 00152: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1372   \n",
      "Epoch 154/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1343Epoch 00153: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1343   \n",
      "Epoch 155/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1362Epoch 00154: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1362   \n",
      "Epoch 156/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1378Epoch 00155: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1378   \n",
      "Epoch 157/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1372Epoch 00156: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1372   \n",
      "Epoch 158/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1371Epoch 00157: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1371   \n",
      "Epoch 159/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1268Epoch 00158: loss improved from 1.13007 to 1.12680, saving model to 1998_char-158-1.1268.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1268   \n",
      "Epoch 160/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1314Epoch 00159: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1314   \n",
      "Epoch 161/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1359Epoch 00160: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1359   \n",
      "Epoch 162/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1362Epoch 00161: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1362   \n",
      "Epoch 163/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1293Epoch 00162: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1293   \n",
      "Epoch 164/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1362Epoch 00163: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1362   \n",
      "Epoch 165/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1356Epoch 00164: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1356   \n",
      "Epoch 166/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1349Epoch 00165: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1349   \n",
      "Epoch 167/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1334Epoch 00166: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1334   \n",
      "Epoch 168/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1339Epoch 00167: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1339   \n",
      "Epoch 169/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1334Epoch 00168: loss did not improve\n",
      "1988627/1988627 [==============================] - 926s - loss: 1.1334   \n",
      "Epoch 170/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1336Epoch 00169: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1336   \n",
      "Epoch 171/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1331Epoch 00170: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1331   \n",
      "Epoch 172/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1318Epoch 00171: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1318   \n",
      "Epoch 173/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1323Epoch 00172: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1323   \n",
      "Epoch 174/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1319Epoch 00173: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1319   \n",
      "Epoch 175/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1315Epoch 00174: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1314   \n",
      "Epoch 176/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1316Epoch 00175: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1316   \n",
      "Epoch 177/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1313Epoch 00176: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1313   \n",
      "Epoch 178/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1300Epoch 00177: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1300   \n",
      "Epoch 179/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1301Epoch 00178: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1301   \n",
      "Epoch 180/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1294Epoch 00179: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1294   \n",
      "Epoch 181/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1306Epoch 00180: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1306   \n",
      "Epoch 182/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1296Epoch 00181: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1296   \n",
      "Epoch 183/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1300Epoch 00182: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1300   \n",
      "Epoch 184/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1292Epoch 00183: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1292   \n",
      "Epoch 185/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1290Epoch 00184: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1290   \n",
      "Epoch 186/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1293Epoch 00185: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1293   \n",
      "Epoch 187/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1281Epoch 00186: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1281   \n",
      "Epoch 188/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1283Epoch 00187: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1283   \n",
      "Epoch 189/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1277Epoch 00188: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1277   \n",
      "Epoch 190/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1278Epoch 00189: loss did not improve\n",
      "1988627/1988627 [==============================] - 922s - loss: 1.1278   \n",
      "Epoch 191/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1279Epoch 00190: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1279   \n",
      "Epoch 192/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1274Epoch 00191: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1274   \n",
      "Epoch 193/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1267Epoch 00192: loss improved from 1.12680 to 1.12668, saving model to 1998_char-192-1.1267.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1267   \n",
      "Epoch 194/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1263Epoch 00193: loss improved from 1.12668 to 1.12625, saving model to 1998_char-193-1.1263.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1263   \n",
      "Epoch 195/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1266Epoch 00194: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1266   \n",
      "Epoch 196/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1262Epoch 00195: loss improved from 1.12625 to 1.12623, saving model to 1998_char-195-1.1262.hdf5\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1262   \n",
      "Epoch 197/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1257Epoch 00196: loss improved from 1.12623 to 1.12570, saving model to 1998_char-196-1.1257.hdf5\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1257   \n",
      "Epoch 198/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1259Epoch 00197: loss did not improve\n",
      "1988627/1988627 [==============================] - 924s - loss: 1.1259   \n",
      "Epoch 199/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1260Epoch 00198: loss did not improve\n",
      "1988627/1988627 [==============================] - 925s - loss: 1.1260   \n",
      "Epoch 200/200\n",
      "1988608/1988627 [============================>.] - ETA: 0s - loss: 1.1260Epoch 00199: loss did not improve\n",
      "1988627/1988627 [==============================] - 923s - loss: 1.1260   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0dc9eeda58>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=256, nb_epoch=200, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ccorbi/Work/Beagle/training/LSTM\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " random.randint(0, len(word_to_id) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in  random.randint(0, len(word_to_id) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37609"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id['genetics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my seed\n",
    "seed = ['a','novel', 'study', 'on', 'human' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "title_ids = return_ids(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35340, 44721, 42855, 15283, 20841]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.zeros((1, max_len, len(word_to_id)))\n",
    "for t, word in enumerate(title_ids):\n",
    "    x[0, t, word] =1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65659,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38887"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature = 1\n",
    "preds = np.asarray(pred).astype('float64')\n",
    "preds = np.log(preds) / temperature\n",
    "exp_preds = np.exp(preds)\n",
    "preds = exp_preds / np.sum(exp_preds)\n",
    "probas = np.random.multinomial(1, preds, 1)\n",
    "np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'large'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[38887]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'heart'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed2 = ['novel', 'study', 'on', 'human','large' ]\n",
    "title_ids = return_ids(seed2)\n",
    "\n",
    "x = np.zeros((1, max_len, len(word_to_id)))\n",
    "for t, word in enumerate(title_ids):\n",
    "    x[0, t, word] =1 \n",
    "pred = model.predict(x)[0]\n",
    "temperature = 1\n",
    "preds = np.asarray(pred).astype('float64')\n",
    "preds = np.log(preds) / temperature\n",
    "exp_preds = np.exp(preds)\n",
    "preds = exp_preds / np.sum(exp_preds)\n",
    "probas = np.random.multinomial(1, preds, 1)\n",
    "id_to_word[np.argmax(probas)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_index = random.randint(0, len(raw_text) - max_len - 1)\n",
    "#len(word_to_id)\n",
    "import gc\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print()\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = raw_text[start_index: start_index + maxlen]\n",
    "    generated += ' '.join([value for value in sentence])\n",
    "    print('----- Generating with seed: \"' + ' '.join([value for value in sentence]) + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, max_len, len(word_to_id)))\n",
    "        for t, word in enumerate(sentence):\n",
    "            x[0, t, char_indices[word]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char + \" \"\n",
    "        sentence = sentence[1:]\n",
    "        sentence.append(next_char)\n",
    "\n",
    "        sys.stdout.write(\" \" + next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
